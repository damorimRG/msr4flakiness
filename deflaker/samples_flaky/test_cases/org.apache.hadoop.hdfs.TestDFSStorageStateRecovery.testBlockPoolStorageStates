/**
 * This test iterates over the testCases table for block pool storage and
 * attempts to startup the DataNode normally.
 */
@Test
public void testBlockPoolStorageStates() throws Exception {
    String[] baseDirs;
    // First setup the datanode storage directory
    String bpid = UpgradeUtilities.getCurrentBlockPoolID(null);
    for (int numDirs = 1; numDirs <= 2; numDirs++) {
        conf = new HdfsConfiguration();
        conf.setInt("dfs.datanode.scan.period.hours", -1);
        conf = UpgradeUtilities.initializeStorageStateConf(numDirs, conf);
        for (int i = 0; i < NUM_DN_TEST_CASES; i++) {
            boolean[] testCase = testCases[i];
            boolean shouldRecover = testCase[SHOULD_RECOVER];
            boolean curAfterRecover = testCase[CURRENT_SHOULD_EXIST_AFTER_RECOVER];
            boolean prevAfterRecover = testCase[PREVIOUS_SHOULD_EXIST_AFTER_RECOVER];
            log("BLOCK_POOL recovery", numDirs, i, testCase);
            createNameNodeStorageState(new boolean[] { true, true, false, false, false });
            cluster = createCluster(conf);
            baseDirs = createBlockPoolStorageState(bpid, testCase);
            if (!testCase[CURRENT_EXISTS] && !testCase[PREVIOUS_EXISTS] && !testCase[PREVIOUS_TMP_EXISTS] && !testCase[REMOVED_TMP_EXISTS]) {
                // DataNode will create and format current if no directories exist
                cluster.startDataNodes(conf, 1, false, StartupOption.REGULAR, null);
            } else {
                if (shouldRecover) {
                    cluster.startDataNodes(conf, 1, false, StartupOption.REGULAR, null);
                    checkResultBlockPool(baseDirs, curAfterRecover, prevAfterRecover);
                } else {
                    cluster.startDataNodes(conf, 1, false, StartupOption.REGULAR, null);
                    assertFalse(cluster.getDataNodes().get(0).isBPServiceAlive(bpid));
                }
            }
            cluster.shutdown();
        }
    // end testCases loop
    }
// end numDirs loop
}
